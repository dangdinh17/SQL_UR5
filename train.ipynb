{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inport env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import SQL vẻr1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from collections import deque\n",
    "\n",
    "# class SoftQNetwork(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "#         super(SoftQNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "#     def forward(self, state):\n",
    "#         x = F.relu(self.fc1(state))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         return self.fc3(x)\n",
    "\n",
    "# class SoftQLearningAgent:\n",
    "#     def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, tau=0.005, \n",
    "#                  alpha=1, buffer_size=10000, batch_size=64, device='cpu'):\n",
    "#         self.state_dim = state_dim\n",
    "#         self.action_dim = action_dim\n",
    "#         self.gamma = gamma\n",
    "#         self.tau = tau\n",
    "#         self.alpha = alpha  # Entropy temperature parameter\n",
    "#         self.batch_size = batch_size\n",
    "#         self.device = device  # CUDA or CPU\n",
    "        \n",
    "#         # Q networks\n",
    "#         self.q_network = SoftQNetwork(state_dim, action_dim).to(device)\n",
    "#         self.target_q_network = SoftQNetwork(state_dim, action_dim).to(device)\n",
    "#         self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "#         self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "#         # Replay buffer\n",
    "#         self.memory = deque(maxlen=buffer_size)\n",
    "        \n",
    "#     def choose_action(self, state, explore=True):\n",
    "#         state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "#         with torch.no_grad():\n",
    "#             q_values = self.q_network(state)\n",
    "        \n",
    "#         if explore:\n",
    "#             # Soft policy: use softmax with temperature\n",
    "#             action_probs = F.softmax(q_values / self.alpha, dim=1)\n",
    "#             action = torch.multinomial(action_probs, 1).squeeze().cpu().numpy()\n",
    "            \n",
    "#             # Generate a full action array with randomness\n",
    "#             full_action = np.zeros(self.action_dim, dtype=np.float32)\n",
    "#             full_action[:-1] = np.random.uniform(-1, 1, self.action_dim-1)\n",
    "#             full_action[-1] = np.random.uniform(-1, 1)\n",
    "            \n",
    "#             return full_action\n",
    "#         else:\n",
    "#             # Deterministic action selection\n",
    "#             action = np.zeros(self.action_dim, dtype=np.float32)\n",
    "#             action[:-1] = np.clip(q_values.squeeze().cpu().numpy()[:-1], -1, 1)\n",
    "#             action[-1] = np.clip(q_values.squeeze().cpu().numpy()[-1], -1, 1)\n",
    "#             return action\n",
    "    \n",
    "#     def compute_entropy(self, q_values):\n",
    "#         # Compute entropy of action distribution\n",
    "#         action_probs = F.softmax(q_values / self.alpha, dim=1)\n",
    "#         log_probs = F.log_softmax(q_values / self.alpha, dim=1)\n",
    "#         entropy = -(action_probs * log_probs).sum(dim=1)\n",
    "#         return entropy\n",
    "    \n",
    "#     def store_transition(self, state, action, reward, next_state, done):\n",
    "#         self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "#     def learn(self):\n",
    "#         if len(self.memory) < self.batch_size:\n",
    "#             return\n",
    "        \n",
    "#         # Sample batch\n",
    "#         batch = random.sample(self.memory, self.batch_size)\n",
    "#         states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "#         states = torch.FloatTensor(states).to(self.device)\n",
    "#         actions = torch.FloatTensor(actions).to(self.device)\n",
    "#         rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "#         next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "#         dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "#         # Current Q-values and entropy\n",
    "#         current_q_values = self.q_network(states)\n",
    "#         current_entropy = self.compute_entropy(current_q_values)\n",
    "        \n",
    "#         # Target Q-values with entropy regularization\n",
    "#         with torch.no_grad():\n",
    "#             next_q_values = self.target_q_network(next_states)\n",
    "#             next_entropy = self.compute_entropy(next_q_values)\n",
    "            \n",
    "#             # Soft Bellman backup\n",
    "#             soft_target_values = rewards + (1 - dones) * self.gamma * (\n",
    "#                 torch.max(next_q_values, dim=1)[0] + self.alpha * next_entropy\n",
    "#             )\n",
    "        \n",
    "#         # Q-value loss with entropy regularization\n",
    "#         q_loss = F.mse_loss(\n",
    "#             torch.sum(current_q_values * actions, dim=1), \n",
    "#             soft_target_values\n",
    "#         )\n",
    "        \n",
    "#         # Optional: Add entropy bonus to encourage exploration\n",
    "#         entropy_loss = torch.mean(current_entropy)\n",
    "        \n",
    "#         # Combined loss\n",
    "#         loss = q_loss + self.alpha * entropy_loss\n",
    "        \n",
    "#         # Optimize\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "        \n",
    "#         # Soft update of target network\n",
    "#         for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n",
    "#             target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "#     def save_model(self):\n",
    "#         torch.save(self.q_network.state_dict(), 'saved_rl_models/best_sql.pt')\n",
    "# def train_soft_q_learning(env, num_episodes=100000, max_steps=200, device=torch.device('cpu')):\n",
    "#     state_dim = env.observation_space.shape[0]\n",
    "#     action_dim = env.action_space.shape[0]\n",
    "    \n",
    "#     agent = SoftQLearningAgent(state_dim, action_dim, device=device)\n",
    "    \n",
    "#     for episode in range(num_episodes):\n",
    "#         state = env.reset()\n",
    "#         total_reward = 0\n",
    "        \n",
    "#         for step in range(max_steps):\n",
    "#             action = agent.choose_action(state)\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "#             agent.store_transition(state, action, reward, next_state, done)\n",
    "#             agent.learn()\n",
    "            \n",
    "#             state = next_state\n",
    "#             total_reward += reward\n",
    "            \n",
    "#             if done:\n",
    "#                 print('done')\n",
    "#                 agent.save_model()\n",
    "#                 break\n",
    "        \n",
    "#         print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "    \n",
    "#     return agent\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     env = ur5GymEnv(renders=True)  # Your custom environment\n",
    "#     trained_agent = train_soft_q_learning(env, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SQL ver1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "# from env import *\n",
    "# from torch.distributions.categorical import Categorical\n",
    "\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, capacity):\n",
    "#         self.capacity = capacity\n",
    "#         self.buffer = []\n",
    "#         self.position = 0\n",
    "\n",
    "#     def push(self, state, action, reward, next_state, done):\n",
    "#         if len(self.buffer) < self.capacity:\n",
    "#             self.buffer.append(None)\n",
    "#         self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         batch = random.sample(self.buffer, batch_size)\n",
    "#         state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "#         return state, action, reward, next_state, done\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.buffer)\n",
    "\n",
    "# class SoftQNetwork(nn.Module):\n",
    "#     def __init__(self, state_dim, num_actions, alpha):\n",
    "#         super(SoftQNetwork, self).__init__()\n",
    "\n",
    "#         self.linear1 = nn.Linear(state_dim, 128)\n",
    "#         self.linear2 = nn.Linear(128, 64)\n",
    "#         self.linear3 = nn.Linear(64, num_actions)\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def get_Q(self, state):\n",
    "#         x = F.relu(self.linear1(state))\n",
    "#         x = F.relu(self.linear2(x))\n",
    "#         x = self.linear3(x)\n",
    "#         return x\n",
    "\n",
    "#     def get_V(self, q):\n",
    "#         # print(q)\n",
    "#         # print(q.shape)\n",
    "#         v = self.alpha * torch.log(torch.mean(torch.exp(q / self.alpha), dim=-1))\n",
    "#         return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import torch.nn.functional as F\n",
    "# from env import ur5GymEnv\n",
    "\n",
    "# def train_soft_q_learning(env, num_episodes=1000, batch_size=64, gamma=0.99, \n",
    "#                            alpha=1, learning_rate=1e-3, buffer_capacity=10000):\n",
    "#     # Get state and action dimensions\n",
    "#     state_dim = env.observation_space.shape[0]\n",
    "#     num_actions = env.action_space.shape[0]\n",
    "\n",
    "#     # Initialize networks\n",
    "#     q_network = SoftQNetwork(state_dim, num_actions, alpha)\n",
    "#     target_q_network = SoftQNetwork(state_dim, num_actions, alpha)\n",
    "#     target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "#     # Optimizer\n",
    "#     optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "#     # Replay buffer\n",
    "#     replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "#     # Training loop\n",
    "#     episode_rewards = []\n",
    "#     for episode in range(num_episodes):\n",
    "#         state = env.reset()\n",
    "#         state = torch.FloatTensor(state)\n",
    "#         total_reward = 0\n",
    "#         done = False\n",
    "\n",
    "#         while not done:\n",
    "#             # Select action\n",
    "#             with torch.no_grad():\n",
    "#                 q_values = q_network.get_Q(state)\n",
    "#                 action = q_values.numpy()  # Use Q-values directly as action\n",
    "#                 action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "\n",
    "#             # Step environment\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "#             next_state = torch.FloatTensor(next_state)\n",
    "            \n",
    "#             # Store transition\n",
    "#             replay_buffer.push(state.numpy(), action, reward, next_state.numpy(), done)\n",
    "#             state = next_state\n",
    "#             total_reward += reward\n",
    "\n",
    "#             # Learn from replay buffer\n",
    "#             if len(replay_buffer) >= batch_size:\n",
    "#                 # Sample batch\n",
    "#                 states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                \n",
    "#                 # Convert to tensors\n",
    "#                 states = torch.FloatTensor(states)\n",
    "#                 next_states = torch.FloatTensor(next_states)\n",
    "#                 actions = torch.FloatTensor(actions)\n",
    "#                 rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "#                 dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "#                 # Compute current Q values\n",
    "#                 current_q_values = q_network.get_Q(states)\n",
    "                \n",
    "#                 # Compute target Q values\n",
    "#                 with torch.no_grad():\n",
    "#                     next_q_values = target_q_network.get_Q(next_states)\n",
    "#                     next_v_value = target_q_network.get_V(next_q_values).unsqueeze(1)\n",
    "                \n",
    "#                 # Compute target\n",
    "#                 target = rewards + gamma  * next_v_value\n",
    "                \n",
    "#                 # Soft Q-learning loss\n",
    "#                 q_loss = F.mse_loss(current_q_values, target)\n",
    "                \n",
    "#                 # Optimize\n",
    "#                 optimizer.zero_grad()\n",
    "#                 q_loss.backward()\n",
    "#                 optimizer.step()\n",
    "#         if done:\n",
    "#             # print('Done')\n",
    "#             torch.save(q_network.state_dict(), 'ur5e_soft_q_model.pth')\n",
    "\n",
    "#         # Soft update of target network\n",
    "#         for target_param, param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "#             target_param.data.copy_(0.005 * param.data + 0.995 * target_param.data)\n",
    "\n",
    "#         episode_rewards.append(total_reward)\n",
    "        \n",
    "#         # Print progress\n",
    "#         # if episode % 10 == 0:\n",
    "#         print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "#     return episode_rewards, q_network\n",
    "\n",
    "# def main():\n",
    "#     render = False\n",
    "#     # Create environment\n",
    "#     env = ur5GymEnv(renders=render)  # Set renders=True to visualize training\n",
    "    \n",
    "#     # Train agent\n",
    "#     rewards, trained_model = train_soft_q_learning(env)\n",
    "    \n",
    "#     # Plot rewards\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     plt.plot(rewards)\n",
    "#     plt.title('Rewards per Episode')\n",
    "#     plt.xlabel('Episode')\n",
    "#     plt.ylabel('Total Reward')\n",
    "#     plt.show()\n",
    "\n",
    "#     # Save model\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train SQL ver2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "from env import *\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "# Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_layer = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mean = self.mean_layer(x)\n",
    "        log_std = self.log_std_layer(x).clamp(-20, 2)  # Clamp log_std for numerical stability\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.rsample()  # Reparameterization trick\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n",
    "        return (\n",
    "            torch.FloatTensor(states),\n",
    "            torch.FloatTensor(actions),\n",
    "            torch.FloatTensor(rewards).unsqueeze(-1),\n",
    "            torch.FloatTensor(next_states),\n",
    "            torch.FloatTensor(dones).unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "# Soft Q-Learning implementation\n",
    "class SoftQLearning:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, gamma=0.99, tau=0.005, alpha=0.2, buffer_size=100000):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.policy_network = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "        self.q_optimizer = optim.Adam(self.q_network.parameters(), lr=3e-4)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=3e-4)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.q_losses = []\n",
    "        self.policy_losses = []\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def update(self, batch_size):\n",
    "        # Sample from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        # Update Q-function\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.policy_network.sample_action(next_states)\n",
    "            target_q = self.target_q_network(next_states, next_actions)\n",
    "            target_value = rewards + (1 - dones) * self.gamma * (target_q - self.alpha * next_log_probs)\n",
    "\n",
    "        q_values = self.q_network(states, actions)\n",
    "        q_loss = ((q_values - target_value) ** 2).mean()\n",
    "        self.q_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        # Update policy\n",
    "        new_actions, log_probs = self.policy_network.sample_action(states)\n",
    "        q_new_actions = self.q_network(states, new_actions)\n",
    "        policy_loss = (self.alpha * log_probs - q_new_actions).mean()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        self.policy_losses.append(policy_loss.item())\n",
    "        self.q_losses.append(q_loss.item())\n",
    "        # Update target Q-network\n",
    "        for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add((state, action, reward, next_state, done))\n",
    "        \n",
    "    # def draws(self)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import gym\n",
    "\n",
    "    env = ur5GymEnv(renders=False)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    agent = SoftQLearning(state_dim, action_dim)\n",
    "\n",
    "    num_episodes = 5000\n",
    "    batch_size = 64\n",
    "    episode_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(200):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action, _ = agent.policy_network.sample_action(state_tensor)\n",
    "            action = action.detach().numpy()[0]\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.add_experience(state, action, reward, next_state, float(done))\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(agent.replay_buffer.buffer) > batch_size:\n",
    "                agent.update(batch_size)\n",
    "\n",
    "            if done:\n",
    "                torch.save(agent.q_network.state_dict(), 'saved_rl_models/ur5e_ver2_model.pth')\n",
    "                break\n",
    "\n",
    "        agent.episode_rewards.append(episode_reward)  # Lưu episode reward\n",
    "        print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
    "\n",
    "    # Vẽ đồ thị sau khi huấn luyện\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Đồ thị Episode Reward\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(agent.episode_rewards, label=\"Episode Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Rewards\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Đồ thị Q Loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(agent.q_losses, label=\"Q Loss\")\n",
    "    plt.xlabel(\"Update Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Q Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Đồ thị Policy Loss\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(agent.policy_losses, label=\"Policy Loss\")\n",
    "    plt.xlabel(\"Update Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Policy Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
