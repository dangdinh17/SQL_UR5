{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\anaconda3\\envs\\rl\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_8020\\2212861093.py:83: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor(states).to(self.device)\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_8020\\2212861093.py:104: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  q_loss = F.mse_loss(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Episode 1/100000, Total Reward: [-527.9111537]\n",
      "done\n",
      "Episode 2/100000, Total Reward: [-581.71515909]\n",
      "done\n",
      "Episode 3/100000, Total Reward: [-505.65371377]\n",
      "done\n",
      "Episode 4/100000, Total Reward: [-393.8973314]\n",
      "done\n",
      "Episode 5/100000, Total Reward: [-872.29759583]\n",
      "done\n",
      "Episode 6/100000, Total Reward: [-1217.93712457]\n",
      "done\n",
      "Episode 7/100000, Total Reward: [-436.2340415]\n",
      "done\n",
      "Episode 8/100000, Total Reward: [-504.23521468]\n",
      "done\n",
      "Episode 9/100000, Total Reward: [-420.25886216]\n",
      "done\n",
      "Episode 10/100000, Total Reward: [-772.28532407]\n",
      "done\n",
      "Episode 11/100000, Total Reward: [-855.41307724]\n",
      "done\n",
      "Episode 12/100000, Total Reward: [-855.851159]\n",
      "done\n",
      "Episode 13/100000, Total Reward: [-895.14541605]\n",
      "done\n",
      "Episode 14/100000, Total Reward: [-1089.83684739]\n",
      "done\n",
      "Episode 15/100000, Total Reward: [-527.30338838]\n",
      "done\n",
      "Episode 16/100000, Total Reward: [-475.81352165]\n",
      "done\n",
      "Episode 17/100000, Total Reward: [-671.35051102]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 159\u001b[0m\n\u001b[0;32m    157\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    158\u001b[0m env \u001b[38;5;241m=\u001b[39m ur5GymEnv(renders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Your custom environment\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m trained_agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_soft_q_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 138\u001b[0m, in \u001b[0;36mtrain_soft_q_learning\u001b[1;34m(env, num_episodes, max_steps, device)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m    137\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[1;32m--> 138\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n\u001b[0;32m    141\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlearn()\n",
      "File \u001b[1;32md:\\SQL_UR5\\env.py:213\u001b[0m, in \u001b[0;36mur5GymEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactionRepeat):\n\u001b[0;32m    212\u001b[0m     p\u001b[38;5;241m.\u001b[39mstepSimulation()\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenders: \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m240.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetExtendedObservation()\n\u001b[0;32m    216\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_reward() \u001b[38;5;66;03m# call this after getting obs!\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class SoftQLearningAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, tau=0.005, \n",
    "                 alpha=1, buffer_size=10000, batch_size=64, device='cpu'):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha  # Entropy temperature parameter\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device  # CUDA or CPU\n",
    "        \n",
    "        # Q networks\n",
    "        self.q_network = SoftQNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_q_network = SoftQNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        \n",
    "    def choose_action(self, state, explore=True):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        \n",
    "        if explore:\n",
    "            # Soft policy: use softmax with temperature\n",
    "            action_probs = F.softmax(q_values / self.alpha, dim=1)\n",
    "            action = torch.multinomial(action_probs, 1).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Generate a full action array with randomness\n",
    "            full_action = np.zeros(self.action_dim, dtype=np.float32)\n",
    "            full_action[:-1] = np.random.uniform(-1, 1, self.action_dim-1)\n",
    "            full_action[-1] = np.random.uniform(-1, 1)\n",
    "            \n",
    "            return full_action\n",
    "        else:\n",
    "            # Deterministic action selection\n",
    "            action = np.zeros(self.action_dim, dtype=np.float32)\n",
    "            action[:-1] = np.clip(q_values.squeeze().cpu().numpy()[:-1], -1, 1)\n",
    "            action[-1] = np.clip(q_values.squeeze().cpu().numpy()[-1], -1, 1)\n",
    "            return action\n",
    "    \n",
    "    def compute_entropy(self, q_values):\n",
    "        # Compute entropy of action distribution\n",
    "        action_probs = F.softmax(q_values / self.alpha, dim=1)\n",
    "        log_probs = F.log_softmax(q_values / self.alpha, dim=1)\n",
    "        entropy = -(action_probs * log_probs).sum(dim=1)\n",
    "        return entropy\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Current Q-values and entropy\n",
    "        current_q_values = self.q_network(states)\n",
    "        current_entropy = self.compute_entropy(current_q_values)\n",
    "        \n",
    "        # Target Q-values with entropy regularization\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_q_network(next_states)\n",
    "            next_entropy = self.compute_entropy(next_q_values)\n",
    "            \n",
    "            # Soft Bellman backup\n",
    "            soft_target_values = rewards + (1 - dones) * self.gamma * (\n",
    "                torch.max(next_q_values, dim=1)[0] + self.alpha * next_entropy\n",
    "            )\n",
    "        \n",
    "        # Q-value loss with entropy regularization\n",
    "        q_loss = F.mse_loss(\n",
    "            torch.sum(current_q_values * actions, dim=1), \n",
    "            soft_target_values\n",
    "        )\n",
    "        \n",
    "        # Optional: Add entropy bonus to encourage exploration\n",
    "        entropy_loss = torch.mean(current_entropy)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = q_loss + self.alpha * entropy_loss\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Soft update of target network\n",
    "        for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.q_network.state_dict(), 'saved_rl_models/best_sql.pt')\n",
    "def train_soft_q_learning(env, num_episodes=100000, max_steps=200, device=torch.device('cpu')):\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    agent = SoftQLearningAgent(state_dim, action_dim, device=device)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            agent.learn()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                print('done')\n",
    "                agent.save_model()\n",
    "                break\n",
    "        \n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    env = ur5GymEnv(renders=True)  # Your custom environment\n",
    "    trained_agent = train_soft_q_learning(env, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\anaconda3\\envs\\rl\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  [0.16830368 0.1689492  0.18071607 0.48203102]\n",
      "action:  [0.16796921 0.16886152 0.18069643 0.4824728 ]\n",
      "action:  [0.16763812 0.16837768 0.18005356 0.4839306 ]\n",
      "action:  [0.1662529  0.16633344 0.17807178 0.48934188]\n",
      "action:  [0.166573   0.16722819 0.17960267 0.4865962 ]\n",
      "action:  [0.1660001  0.16640314 0.17845926 0.48913753]\n",
      "action:  [0.16501935 0.16404252 0.17498639 0.4959518 ]\n",
      "action:  [0.16483834 0.16248028 0.17172404 0.50095737]\n",
      "action:  [0.1647227  0.16199186 0.17108999 0.5021955 ]\n",
      "action:  [0.16480145 0.1616091  0.17099853 0.502591  ]\n",
      "action:  [0.16410933 0.16088404 0.1693797  0.505627  ]\n",
      "action:  [0.16445436 0.16052337 0.16973592 0.50528634]\n",
      "action:  [0.16485702 0.16082394 0.17085885 0.50346017]\n",
      "action:  [0.1638408  0.16105983 0.17206727 0.5030321 ]\n",
      "action:  [0.1624472  0.15991025 0.17041087 0.50723165]\n",
      "action:  [0.16230807 0.15868612 0.16831113 0.5106947 ]\n",
      "action:  [0.16106854 0.15823193 0.16804372 0.51265585]\n",
      "action:  [0.16050555 0.15837671 0.16903563 0.51208216]\n",
      "action:  [0.16013807 0.15844744 0.16960202 0.5118125 ]\n",
      "action:  [0.15923324 0.1584623  0.17071845 0.51158607]\n",
      "action:  [0.15887117 0.15802537 0.17019872 0.5129047 ]\n",
      "action:  [0.15927443 0.15725768 0.16800477 0.5154632 ]\n",
      "action:  [0.15932418 0.15731084 0.16820087 0.51516414]\n",
      "action:  [0.15813664 0.15570195 0.16591777 0.5202437 ]\n",
      "action:  [0.15789014 0.15599996 0.16711596 0.5189939 ]\n",
      "action:  [0.15731058 0.15470314 0.16508286 0.52290344]\n",
      "action:  [0.15706052 0.15513009 0.1663428  0.5214666 ]\n",
      "action:  [0.15716499 0.15461437 0.1652969  0.52292377]\n",
      "action:  [0.15635115 0.15441993 0.16573957 0.5234894 ]\n",
      "action:  [0.15619147 0.15328714 0.16339839 0.527123  ]\n",
      "action:  [0.15703149 0.15422377 0.16500276 0.52374196]\n",
      "action:  [0.15497603 0.15215781 0.16261598 0.53025025]\n",
      "action:  [0.1542873  0.15102322 0.1610165  0.53367305]\n",
      "action:  [0.15386616 0.15027462 0.1603039  0.5355553 ]\n",
      "action:  [0.15267645 0.15057299 0.16255517 0.5341955 ]\n",
      "action:  [0.15222797 0.1501447  0.16216524 0.5354621 ]\n",
      "action:  [0.15244737 0.14915097 0.1596584  0.53874326]\n",
      "action:  [0.15171307 0.14937834 0.16115247 0.53775615]\n",
      "action:  [0.15138516 0.14942865 0.16165029 0.5375359 ]\n",
      "action:  [0.15095463 0.1493474  0.16208532 0.5376127 ]\n",
      "action:  [0.15058945 0.14851123 0.1607916  0.5401077 ]\n",
      "action:  [0.15059598 0.14740387 0.15854019 0.54346   ]\n",
      "action:  [0.15065661 0.14685658 0.1576643  0.5448225 ]\n",
      "action:  [0.15043364 0.14625068 0.15671496 0.5466007 ]\n",
      "action:  [0.14990327 0.14576823 0.15594637 0.54838216]\n",
      "action:  [0.14896443 0.14574857 0.15692985 0.5483572 ]\n",
      "action:  [0.14887989 0.14610572 0.15768044 0.547334  ]\n",
      "action:  [0.14859676 0.1465587  0.15894389 0.54590064]\n",
      "action:  [0.14846611 0.14683434 0.15965335 0.54504627]\n",
      "action:  [0.14826211 0.14668629 0.15959486 0.5454567 ]\n",
      "action:  [0.14745362 0.14605126 0.15931469 0.5471804 ]\n",
      "action:  [0.14708681 0.14640762 0.16051851 0.54598707]\n",
      "action:  [0.14727001 0.14633778 0.16061187 0.54578036]\n",
      "action:  [0.14865223 0.14638418 0.16054629 0.5444173 ]\n",
      "action:  [0.14853512 0.14591612 0.15994574 0.54560304]\n",
      "action:  [0.14735538 0.14545926 0.159975   0.54721034]\n",
      "action:  [0.14679019 0.1451605  0.15988804 0.54816127]\n",
      "action:  [0.14660431 0.14471929 0.15931657 0.5493598 ]\n",
      "action:  [0.14685774 0.14447755 0.15923165 0.5494331 ]\n",
      "action:  [0.1463054  0.14398058 0.15854575 0.5511683 ]\n",
      "action:  [0.14548104 0.14353763 0.15787974 0.5531016 ]\n",
      "action:  [0.14504872 0.14310879 0.15697971 0.55486286]\n",
      "action:  [0.14370374 0.14180931 0.15520702 0.5592799 ]\n",
      "action:  [0.14302787 0.14061616 0.15354773 0.5628082 ]\n",
      "action:  [0.14289647 0.14127906 0.1549175  0.560907  ]\n",
      "torch.Size([64, 4])\n",
      "[0.15171307 0.14937834 0.16115247 0.53775615]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 124\u001b[0m\n\u001b[0;32m    122\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39mpush(state, action, reward, next_state, done)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m--> 124\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 90\u001b[0m, in \u001b[0;36mSoftQ.train\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(action)):\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(action[i])\n\u001b[1;32m---> 90\u001b[0m     est_q[i] \u001b[38;5;241m=\u001b[39m reward[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[43mnext_v\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     91\u001b[0m q_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(q, est_q\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_q_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "# from env import *\n",
    "# from torch.distributions.categorical import Categorical\n",
    "\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, capacity):\n",
    "#         self.capacity = capacity\n",
    "#         self.buffer = []\n",
    "#         self.position = 0\n",
    "\n",
    "#     def push(self, state, action, reward, next_state, done):\n",
    "#         if len(self.buffer) < self.capacity:\n",
    "#             self.buffer.append(None)\n",
    "#         self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         batch = random.sample(self.buffer, batch_size)\n",
    "#         state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "#         return state, action, reward, next_state, done\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.buffer)\n",
    "\n",
    "# class SoftQNetwork(nn.Module):\n",
    "#     def __init__(self, state_dim, num_actions, alpha):\n",
    "#         super(SoftQNetwork, self).__init__()\n",
    "\n",
    "#         self.linear1 = nn.Linear(state_dim, 128)\n",
    "#         self.linear2 = nn.Linear(128, 64)\n",
    "#         self.linear3 = nn.Linear(64, num_actions)\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def get_Q(self, state):\n",
    "#         x = F.relu(self.linear1(state))\n",
    "#         x = F.relu(self.linear2(x))\n",
    "#         x = self.linear3(x)\n",
    "#         return x\n",
    "\n",
    "#     def get_V(self, q):\n",
    "#         # print(q)\n",
    "#         # print(q.shape)\n",
    "#         v = self.alpha*torch.log(torch.mean(torch.exp(q/self.alpha)))\n",
    "#         return v\n",
    "\n",
    "# class SoftQ(object):\n",
    "#     def __init__(self, state_dim, action_dim):\n",
    "#         self.alpha = 2\n",
    "#         self.soft_q_net = SoftQNetwork(state_dim, action_dim, self.alpha).to(device)\n",
    "#         self.v_criterion = nn.MSELoss()\n",
    "#         self.soft_q_criterion = nn.MSELoss()\n",
    "#         self.soft_q_optimizer = optim.Adam(self.soft_q_net.parameters(), lr=1e-3)\n",
    "#         self.gamma = 0.9\n",
    "\n",
    "#     def get_action(self, state):\n",
    "#         py_state = torch.from_numpy(state).float()\n",
    "#         temp_q = self.soft_q_net.get_Q(py_state)\n",
    "#         # print(temp_q)\n",
    "#         dist = torch.exp((temp_q-self.soft_q_net.get_V(temp_q))/self.alpha)\n",
    "#         # print(dist)\n",
    "#         dist = dist / torch.sum(dist)\n",
    "#         # print(dist)\n",
    "#         m = Categorical(dist.squeeze(0))\n",
    "#         a = m.sample()\n",
    "#         return dist\n",
    "\n",
    "#     def train(self, batch):\n",
    "#         state = batch[0]  # array [64 1 2]\n",
    "#         action = batch[1]  # array [64, ]\n",
    "#         reward = batch[2]  # array [64, ]\n",
    "#         next_state = batch[3]\n",
    "#         state = torch.from_numpy(state).float().to(device)\n",
    "#         next_state = torch.from_numpy(next_state).float().to(device)\n",
    "#         reward = torch.FloatTensor(reward).float().to(device)\n",
    "\n",
    "#         q = self.soft_q_net.get_Q(state).squeeze(1)\n",
    "#         est_q = q.clone()\n",
    "#         print(est_q.shape)\n",
    "#         next_q = self.soft_q_net.get_Q(next_state).squeeze(1)\n",
    "#         next_v = self.soft_q_net.get_V(next_q)\n",
    "#         for i in range(len(action)):\n",
    "#             print(action[i])\n",
    "#             est_q[i] = reward[i] + self.gamma * next_v[i]\n",
    "#         q_loss = F.mse_loss(q, est_q.detach())\n",
    "#         self.soft_q_optimizer.zero_grad()\n",
    "#         q_loss.backward()\n",
    "#         self.soft_q_optimizer.step()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     use_cuda = torch.cuda.is_available()\n",
    "#     device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#     env = ur5GymEnv(renders=False)\n",
    "#     state_dim = env.observation_space.shape[0]\n",
    "#     action_dim = env.action_space.shape[0]\n",
    "#     agent = SoftQ(state_dim = state_dim, action_dim = action_dim)\n",
    "#     max_MC_iter = 200\n",
    "#     max_epi_iter = 500\n",
    "#     batch_size = 64\n",
    "#     replay_buffer = ReplayBuffer(10000)\n",
    "#     train_curve = []\n",
    "#     for epi in range(max_epi_iter):\n",
    "#         state = env.reset()\n",
    "#         # state = state.reshape((1, state_dim))\n",
    "#         acc_reward = 0\n",
    "#         for MC_iter in range(max_MC_iter):\n",
    "#             # print(state)\n",
    "            \n",
    "#             action = agent.get_action(state)\n",
    "            \n",
    "#             action = np.array(action.squeeze().detach().cpu())\n",
    "#             print(\"action: \", action)\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "#             acc_reward = acc_reward + reward\n",
    "#             state = next_state\n",
    "#             replay_buffer.push(state, action, reward, next_state, done)\n",
    "#             if len(replay_buffer) > batch_size:\n",
    "#                 agent.train(replay_buffer.sample(batch_size))\n",
    "#             if done:\n",
    "#                 break\n",
    "#         print('Episode', epi, 'reward', acc_reward / MC_iter)\n",
    "#         train_curve.append(acc_reward)\n",
    "#     plt.plot(train_curve, linewidth=1, label='SAC')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
